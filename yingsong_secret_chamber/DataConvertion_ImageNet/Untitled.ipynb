{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code will read the TFRecords \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asynchronously process images using multiple threads specified by\n",
      "the following parameters and the resulting processed images are stored in a\n",
      "random shuffling queue.\n",
      "    \n",
      "FLAGS.batch_size = 32\n",
      "FLAGS.image_size = 299\n",
      "FLAGS.num_preprocess_threads = 4\n",
      "FLAGS.num_readers = 4\n",
      "FLAGS.input_queue_memory_factor = 16\n"
     ]
    }
   ],
   "source": [
    "# this is the set of parameters you want to change during the training to suit your computational need\n",
    "\n",
    "import FLAGS\n",
    "reload(FLAGS)\n",
    "FLAGS.print_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields in a TFRecords example:<br\\>\n",
    "'image/height': _int64_feature(height),<br\\>\n",
    "'image/width': _int64_feature(width),<br\\>\n",
    "'image/colorspace': _bytes_feature(colorspace),<br\\>\n",
    "'image/channels': _int64_feature(channels),<br\\>\n",
    "'image/class/label': _int64_feature(label),<br\\>\n",
    "'image/class/synset': _bytes_feature(synset),<br\\>\n",
    "'image/class/text': _bytes_feature(human),<br\\>\n",
    "'image/object/bbox/xmin': _float_feature(xmin),<br\\>\n",
    "'image/object/bbox/xmax': _float_feature(xmax),<br\\>\n",
    "'image/object/bbox/ymin': _float_feature(ymin),<br\\>\n",
    "'image/object/bbox/ymax': _float_feature(ymax),<br\\>\n",
    "'image/object/bbox/label': _int64_feature([label] * len(xmin)),<br\\>\n",
    "'image/format': _bytes_feature(image_format),<br\\>\n",
    "'image/filename': _bytes_feature(os.path.basename(filename)),<br\\>\n",
    "'image/encoded': _bytes_feature(image_buffer)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_jpeg(image_buffer, scope=None):\n",
    "    \"\"\"Decode a JPEG string into one 3-D float image Tensor.\n",
    "    Args:\n",
    "        image_buffer: scalar string Tensor.\n",
    "        scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "        3-D float Tensor with values ranging from [0, 1).\n",
    "    \"\"\"\n",
    "    with tf.op_scope([image_buffer], scope, 'decode_jpeg'):\n",
    "        # Decode the string as an RGB JPEG.\n",
    "        # Note that the resulting image contains an unknown height and width\n",
    "        # that is set dynamically by decode_jpeg. In other words, the height\n",
    "        # and width of image is unknown at compile-time.\n",
    "        image = tf.image.decode_jpeg(image_buffer, channels=3)\n",
    "\n",
    "        # After this point, all image pixels reside in [0,1)\n",
    "        # until the very end, when they're rescaled to (-1, 1).  The various\n",
    "        # adjust_* ops all require this range for dtype float.\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_preprocessing(image_buffer):\n",
    "    \"\"\"Decode and preprocess one image for evaluation or training.\n",
    "    Args:\n",
    "        image_buffer: JPEG encoded string Tensor\n",
    "        train: boolean\n",
    "        thread_id: integer indicating preprocessing thread\n",
    "    Returns:\n",
    "        3-D float Tensor containing an appropriately scaled image\n",
    "    Raises:\n",
    "        ValueError: if user does not provide bounding box\n",
    "    \"\"\"\n",
    "     \n",
    "\n",
    "    image = decode_jpeg(image_buffer)\n",
    "    height = FLAGS.image_size\n",
    "    width = FLAGS.image_size\n",
    "    # Crop the central region of the image with an area containing 87.5% of\n",
    "    # the original image.\n",
    "    image = tf.image.central_crop(image, central_fraction=0.875)\n",
    "    # Resize the image to the original height and width.\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    image = tf.image.resize_bilinear(image, [height, width],\n",
    "                                     align_corners=False)\n",
    "    image = tf.squeeze(image, [0])\n",
    "\n",
    "    # Finally, rescale to [-1,1] instead of [0, 1)\n",
    "    image = tf.sub(image, 0.5)\n",
    "    image = tf.mul(image, 2.0)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_example_proto(example_serialized):\n",
    "    \"\"\"Parses an Example proto containing a training example of an image.\n",
    "    The output of the build_image_data.py image preprocessing script is a dataset\n",
    "    containing serialized Example protocol buffers. Each Example proto contains\n",
    "    the following fields:\n",
    "        image/height: 462\n",
    "        image/width: 581\n",
    "        image/colorspace: 'RGB'\n",
    "        image/channels: 3\n",
    "        image/class/label: 615\n",
    "        image/class/synset: 'n03623198'\n",
    "        image/class/text: 'knee pad'\n",
    "        image/object/bbox/xmin: 0.1\n",
    "        image/object/bbox/xmax: 0.9\n",
    "        image/object/bbox/ymin: 0.2\n",
    "        image/object/bbox/ymax: 0.6\n",
    "        image/object/bbox/label: 615\n",
    "        image/format: 'JPEG'\n",
    "        image/filename: 'ILSVRC2012_val_00041207.JPEG'\n",
    "        image/encoded: <JPEG encoded string>\n",
    "    Args:\n",
    "        example_serialized: scalar Tensor tf.string containing a serialized\n",
    "        Example protocol buffer.\n",
    "    Returns:\n",
    "        image_buffer: Tensor tf.string containing the contents of a JPEG file.\n",
    "        label: Tensor tf.int32 containing the label.\n",
    "        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "        where each coordinate is [0, 1) and the coordinates are arranged as\n",
    "        [ymin, xmin, ymax, xmax].\n",
    "        text: Tensor tf.string containing the human-readable label.\n",
    "    \"\"\"\n",
    "    # Dense features in Example proto.\n",
    "    feature_map = {\n",
    "        'image/encoded': tf.FixedLenFeature([], dtype=tf.string,\n",
    "                                          default_value=''),\n",
    "        'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64,\n",
    "                                              default_value=-1),\n",
    "        'image/class/text': tf.FixedLenFeature([], dtype=tf.string,\n",
    "                                             default_value=''),\n",
    "    }\n",
    "    \n",
    "    features = tf.parse_single_example(example_serialized, feature_map)\n",
    "    label = tf.cast(features['image/class/label'], dtype=tf.int32)\n",
    "    \n",
    "\n",
    "    \n",
    "    return features['image/encoded'], label, features['image/class/text']\n",
    "\n",
    "def batch_inputs(dataset, batch_size, shuffle, num_preprocess_threads=None,\n",
    "                 num_readers=1):\n",
    "    \"\"\"Contruct batches of training or evaluation examples from the image dataset.\n",
    "    Args:\n",
    "    dataset:  str as either 'train' or 'val'\n",
    "    batch_size: integer\n",
    "    shuffle: boolean\n",
    "    num_preprocess_threads: integer, total number of preprocessing threads\n",
    "    num_readers: integer, number of parallel readers\n",
    "    \n",
    "    Returns:\n",
    "    images: 4-D float Tensor of a batch of images\n",
    "    labels: 1-D integer Tensor of [batch_size].\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: if data is not found\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope('batch_processing'):\n",
    "        # retrieve all the trainning TFRecords in a python list data_file\n",
    "        tf_record_pattern = os.path.join('/home/sherlock/workspace/ILSVRC2015/TFRecords/','%s-*'%dataset)\n",
    "        data_files = tf.gfile.Glob(tf_record_pattern)\n",
    "        data_files = data_files[0:2]\n",
    "        if data_files is None:\n",
    "            raise ValueError('No data files found for this dataset')\n",
    "        # Create filename_queue\n",
    "        if shuffle:\n",
    "            filename_queue = tf.train.string_input_producer(data_files,\n",
    "                                                      shuffle=True,\n",
    "                                                      capacity=16)\n",
    "        else:\n",
    "            print data_files\n",
    "            filename_queue = tf.train.string_input_producer(data_files,\n",
    "                                                      shuffle=False,\n",
    "                                                      capacity=1)\n",
    "        if num_preprocess_threads is None:\n",
    "            num_preprocess_threads = FLAGS.num_preprocess_threads \n",
    "        if num_preprocess_threads % 4:\n",
    "            raise ValueError('Please make num_preprocess_threads a multiple '\n",
    "                               'of 4 (%d % 4 != 0).', num_preprocess_threads)  \n",
    "                \n",
    "        if num_readers is None:\n",
    "            num_readers =  FLAGS.num_readers\n",
    "        if num_readers < 1:\n",
    "            raise ValueError('Please make num_readers at least 1')\n",
    "\n",
    "        # Approximate number of examples per shard. (shard is a partition of the dataset)\n",
    "        examples_per_shard = 1024\n",
    "        # Size the random shuffle queue to balance between good global\n",
    "        # mixing (more examples) and memory use (fewer examples).\n",
    "        # 1 image uses 299*299*3*4 bytes = 1MB\n",
    "        # The default input_queue_memory_factor is 16 implying a shuffling queue\n",
    "        # size: examples_per_shard * 16 * 1MB = 17.6GB\n",
    "        min_queue_examples = examples_per_shard * FLAGS.input_queue_memory_factor\n",
    "        if shuffle:\n",
    "            examples_queue = tf.RandomShuffleQueue(\n",
    "                capacity=min_queue_examples + 3 * batch_size,\n",
    "                min_after_dequeue=min_queue_examples,\n",
    "                dtypes=[tf.string])\n",
    "        else:\n",
    "            print 'FIFO'\n",
    "            examples_queue = tf.FIFOQueue(\n",
    "                capacity=examples_per_shard + 3 * batch_size,\n",
    "                dtypes=[tf.string])\n",
    "\n",
    "        # Create multiple readers to populate the queue of examples.\n",
    "        if num_readers > 1:\n",
    "            enqueue_ops = []\n",
    "            for _ in range(num_readers):\n",
    "                reader = tf.TFRecordReader()\n",
    "                _, value = reader.read(filename_queue)\n",
    "                enqueue_ops.append(examples_queue.enqueue([value]))\n",
    "\n",
    "            tf.train.queue_runner.add_queue_runner(\n",
    "                tf.train.queue_runner.QueueRunner(examples_queue, enqueue_ops))\n",
    "            example_serialized = examples_queue.dequeue()\n",
    "        else:\n",
    "            print 'reader %d' %num_readers\n",
    "            reader = tf.TFRecordReader()\n",
    "            _, example_serialized = reader.read(filename_queue)\n",
    "\n",
    "        images_and_labels_and_text = []\n",
    "        for thread_id in range(num_preprocess_threads):\n",
    "            # Parse a serialized Example proto to extract the image and metadata.\n",
    "            image_buffer, label_index, label_text = parse_example_proto(\n",
    "            example_serialized)\n",
    "            image = image_preprocessing(image_buffer)\n",
    "            images_and_labels_and_text.append([image, label_index,label_text])\n",
    "\n",
    "        images, label_index_batch,label_text_batch = tf.train.batch_join(\n",
    "            images_and_labels_and_text,\n",
    "            batch_size=batch_size,\n",
    "            capacity=2 * num_preprocess_threads * batch_size)\n",
    "\n",
    "    # Reshape images into these desired dimensions.\n",
    "        height = FLAGS.image_size\n",
    "        width = FLAGS.image_size\n",
    "        depth = 3\n",
    "\n",
    "        images = tf.cast(images, tf.float32)\n",
    "        images = tf.reshape(images, shape=[batch_size, height, width, depth])\n",
    "\n",
    "        # Display the training images in the visualizer.\n",
    "        tf.image_summary('images', images)\n",
    "\n",
    "    return images, tf.reshape(label_index_batch, [batch_size]),tf.reshape(label_text_batch, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/sherlock/workspace/ILSVRC2015/TFRecords/validation-00086-of-00128', '/home/sherlock/workspace/ILSVRC2015/TFRecords/validation-00068-of-00128']\n",
      "<tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x7f21aa5f3e10>\n",
      "FIFO\n",
      "reader 1\n",
      "Step 0: number of images = 0 (0.182 sec)\n",
      "Step 10: number of images = 320 (0.207 sec)\n",
      "Step 20: number of images = 640 (0.183 sec)\n",
      "Step 30: number of images = 960 (0.163 sec)\n",
      "Step 40: number of images = 1280 (0.164 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8c3040dfee6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# the list passed to sess.run() and the value tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# will be returned in the tuple from the call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mImage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mImage_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sherlock/anaconda/envs/Python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sherlock/anaconda/envs/Python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sherlock/anaconda/envs/Python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/sherlock/anaconda/envs/Python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sherlock/anaconda/envs/Python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    image,label,text = batch_inputs('validation', FLAGS.batch_size, shuffle=False, \n",
    "                                    num_preprocess_threads=None,\n",
    "                                    num_readers=1)\n",
    "    \n",
    "    # Image_t is a tensor we define to run the session\n",
    "    Image_t = tf.add(image,0)\n",
    "    init_op = tf.group(tf.initialize_all_variables(),\n",
    "                        tf.initialize_local_variables())\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    #\n",
    "\n",
    "    try:\n",
    "        step = 0\n",
    "        num=0\n",
    "        while not coord.should_stop():\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Run one step of the model.  The return values are\n",
    "            # the activations from the `train_op` (which is\n",
    "            # discarded) and the `loss` op.  To inspect the values\n",
    "            # of your ops or variables, you may include them in\n",
    "            # the list passed to sess.run() and the value tensors\n",
    "            # will be returned in the tuple from the call.\n",
    "            Image=sess.run([Image_t])\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Print an overview fairly often.\n",
    "            if step % 10 == 0:\n",
    "                print('Step %d: number of images = %d (%.3f sec)' % (step, num,\n",
    "                                                     duration))\n",
    "            step += 1\n",
    "            num += int(Image_t.get_shape()[0])\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (2, step))\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.92941189,  0.92156875,  0.78039229],\n",
       "         [ 0.94509816,  0.92156875,  0.77984142],\n",
       "         [ 0.94509816,  0.92156875,  0.78823543],\n",
       "         ..., \n",
       "         [ 0.91372561,  0.8901962 ,  0.75686288],\n",
       "         [ 0.90698445,  0.88180208,  0.74901974],\n",
       "         [ 0.92156875,  0.87450993,  0.74901974]],\n",
       "\n",
       "        [[ 0.94509816,  0.92156875,  0.78823543],\n",
       "         [ 0.94509816,  0.92156875,  0.78823543],\n",
       "         [ 0.94509816,  0.92156875,  0.80281997],\n",
       "         ..., \n",
       "         [ 0.90945411,  0.8859247 ,  0.75259137],\n",
       "         [ 0.90666938,  0.88313997,  0.74980664],\n",
       "         [ 0.90666938,  0.88313997,  0.74980664]],\n",
       "\n",
       "        [[ 0.94352424,  0.91999483,  0.78666151],\n",
       "         [ 0.94509816,  0.92156875,  0.78969872],\n",
       "         [ 0.94509816,  0.92156875,  0.8030411 ],\n",
       "         ..., \n",
       "         [ 0.91372561,  0.8901962 ,  0.75686288],\n",
       "         [ 0.91215169,  0.88862228,  0.75528896],\n",
       "         [ 0.91299403,  0.88946462,  0.75613129]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.77018821,  0.58195293,  0.56154466],\n",
       "         [ 0.7766943 ,  0.57103622,  0.55952108],\n",
       "         [ 0.79328907,  0.54932106,  0.55129707],\n",
       "         ..., \n",
       "         [ 0.19373274, -0.68517113, -0.81372947],\n",
       "         [ 0.19953024, -0.67333555, -0.83468211],\n",
       "         [ 0.27385366, -0.65974677, -0.77117312]],\n",
       "\n",
       "        [[ 0.76628006,  0.59685218,  0.57489669],\n",
       "         [ 0.77484047,  0.59626937,  0.5771178 ],\n",
       "         [ 0.76986825,  0.56075251,  0.55393255],\n",
       "         ..., \n",
       "         [ 0.22453034, -0.65138686, -0.7822026 ],\n",
       "         [ 0.16181362, -0.71102601, -0.85889626],\n",
       "         [ 0.24882793, -0.6954841 , -0.79570264]],\n",
       "\n",
       "        [[ 0.77881825,  0.61411238,  0.59058297],\n",
       "         [ 0.79413867,  0.5996151 ,  0.58728361],\n",
       "         [ 0.80610693,  0.6032871 ,  0.5948931 ],\n",
       "         ..., \n",
       "         [ 0.22203267, -0.66331363, -0.79930794],\n",
       "         [ 0.23413205, -0.68342787, -0.79672444],\n",
       "         [ 0.2311728 , -0.67280006, -0.79283422]]],\n",
       "\n",
       "\n",
       "       [[[ 0.52941191,  0.58431387,  0.81176484],\n",
       "         [ 0.06590605,  0.05751204,  0.13284814],\n",
       "         [-0.17631316, -0.02317524,  0.13436949],\n",
       "         ..., \n",
       "         [-0.01753592,  0.04355669,  0.06606257],\n",
       "         [-0.09956288, -0.0280019 ,  0.11605871],\n",
       "         [-0.22352934, -0.06047642, -0.00722748]],\n",
       "\n",
       "        [[ 0.03256619,  0.17550015,  0.23583198],\n",
       "         [ 0.36445487,  0.39390874,  0.42089462],\n",
       "         [-0.10431617, -0.01096869,  0.1160773 ],\n",
       "         ..., \n",
       "         [ 0.09462059,  0.07357073,  0.12399995],\n",
       "         [-0.03294659, -0.01946849,  0.0475477 ],\n",
       "         [-0.01732481,  0.06446254,  0.09770381]],\n",
       "\n",
       "        [[ 0.49502277,  0.51094508,  0.54289472],\n",
       "         [ 0.34062374,  0.36368191,  0.32354033],\n",
       "         [-0.14441979, -0.06863773,  0.03057051],\n",
       "         ..., \n",
       "         [ 0.05853164,  0.16313863,  0.23839593],\n",
       "         [ 0.13890743,  0.11672008,  0.17732513],\n",
       "         [-0.05231637,  0.00920033,  0.05590308]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.16445833, -0.04497421,  0.10624981],\n",
       "         [-0.15005088, -0.11370766, -0.00294369],\n",
       "         [-0.41465849, -0.20336843, -0.02340311],\n",
       "         ..., \n",
       "         [-0.04454666, -0.00887078,  0.07864928],\n",
       "         [-0.18216646, -0.12377208, -0.07634294],\n",
       "         [-0.1542027 , -0.08944064, -0.02635711]],\n",
       "\n",
       "        [[-0.31641722, -0.16241378,  0.05653918],\n",
       "         [-0.1791423 , -0.08261156,  0.07831609],\n",
       "         [-0.24971783, -0.05812699,  0.07520461],\n",
       "         ..., \n",
       "         [-0.00516617,  0.06668353,  0.09416807],\n",
       "         [-0.06802011, -0.03377122,  0.0171535 ],\n",
       "         [-0.19735777, -0.1699037 , -0.12105197]],\n",
       "\n",
       "        [[-0.37606317, -0.237746  , -0.00567859],\n",
       "         [-0.08455038,  0.04941213,  0.24791682],\n",
       "         [-0.26369035, -0.11057931,  0.05779243],\n",
       "         ..., \n",
       "         [ 0.02414703,  0.06390107,  0.06090975],\n",
       "         [ 0.13012052,  0.14743292,  0.17377555],\n",
       "         [-0.19185644, -0.07968229, -0.05443406]]],\n",
       "\n",
       "\n",
       "       [[[ 0.6549021 ,  0.67058837,  0.48235297],\n",
       "         [ 0.67058837,  0.67058837,  0.47910035],\n",
       "         [ 0.67058837,  0.67058837,  0.47317207],\n",
       "         ..., \n",
       "         [ 0.84906566,  0.84715092,  0.81281412],\n",
       "         [ 0.87718594,  0.87584794,  0.8504039 ],\n",
       "         [ 0.8839792 ,  0.88235307,  0.86666679]],\n",
       "\n",
       "        [[ 0.63556969,  0.67478538,  0.43949115],\n",
       "         [ 0.65339303,  0.68345857,  0.45637012],\n",
       "         [ 0.64643705,  0.67210042,  0.46033561],\n",
       "         ..., \n",
       "         [ 0.84700572,  0.84509099,  0.81075418],\n",
       "         [ 0.87987089,  0.87853289,  0.85308886],\n",
       "         [ 0.88484943,  0.87700629,  0.86132002]],\n",
       "\n",
       "        [[ 0.63866496,  0.67788064,  0.42800188],\n",
       "         [ 0.63343573,  0.67253721,  0.43758559],\n",
       "         [ 0.64269686,  0.67508495,  0.44446743],\n",
       "         ..., \n",
       "         [ 0.83393025,  0.83201551,  0.79314244],\n",
       "         [ 0.86664104,  0.86530304,  0.83985913],\n",
       "         [ 0.88246727,  0.87462413,  0.85893786]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.32218528,  0.5199163 ,  0.31743741],\n",
       "         [ 0.32874477,  0.52402222,  0.3153528 ],\n",
       "         [ 0.32383776,  0.52156878,  0.30896127],\n",
       "         ..., \n",
       "         [ 0.46042895,  0.23655593,  0.13026142],\n",
       "         [ 0.48198414,  0.24700451,  0.15482318],\n",
       "         [ 0.43587184,  0.19438708,  0.10811257]],\n",
       "\n",
       "        [[ 0.3176471 ,  0.51826286,  0.29191375],\n",
       "         [ 0.33060503,  0.52392864,  0.29757953],\n",
       "         [ 0.31485879,  0.51120651,  0.28485739],\n",
       "         ..., \n",
       "         [ 0.4591136 ,  0.25128067,  0.12435293],\n",
       "         [ 0.45963621,  0.24721897,  0.14311504],\n",
       "         [ 0.40101612,  0.18140829,  0.0864172 ]],\n",
       "\n",
       "        [[ 0.28826761,  0.44513035,  0.22916865],\n",
       "         [ 0.28664124,  0.43843675,  0.22421551],\n",
       "         [ 0.2756598 ,  0.42412841,  0.21656072],\n",
       "         ..., \n",
       "         [ 0.44169617,  0.24825335,  0.13603616],\n",
       "         [ 0.44719982,  0.23576939,  0.12966585],\n",
       "         [ 0.40392983,  0.17751431,  0.07009256]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 1.        ,  1.        ,  0.93725502],\n",
       "         [ 1.        ,  1.        ,  0.93725502],\n",
       "         [ 1.        ,  1.        ,  0.93725502],\n",
       "         ..., \n",
       "         [-0.72074234, -0.60805279, -0.91558766],\n",
       "         [-0.53780586, -0.5305137 , -0.87230581],\n",
       "         [-0.48633909, -0.53670388, -0.83474314]],\n",
       "\n",
       "        [[ 1.        ,  1.        ,  0.93725502],\n",
       "         [ 1.        ,  1.        ,  0.93725502],\n",
       "         [ 1.        ,  1.        ,  0.93725502],\n",
       "         ..., \n",
       "         [-0.93110919, -0.63150334, -0.91829705],\n",
       "         [-0.79102921, -0.58291036, -0.89303505],\n",
       "         [-0.71247739, -0.59871542, -0.8841216 ]],\n",
       "\n",
       "        [[ 1.        ,  1.        ,  0.93725502],\n",
       "         [ 1.        ,  1.        ,  0.93725502],\n",
       "         [ 1.        ,  1.        ,  0.93725502],\n",
       "         ..., \n",
       "         [-0.88910997, -0.60495055, -0.86288273],\n",
       "         [-0.78932881, -0.64075756, -0.92610633],\n",
       "         [-0.82871872, -0.59653974, -0.90585113]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.7888394 , -0.29183608, -0.41567379],\n",
       "         [-0.88499886, -0.34317791, -0.45684147],\n",
       "         [-0.80551779, -0.30295414, -0.45189482],\n",
       "         ..., \n",
       "         [-1.        , -1.        , -1.        ],\n",
       "         [-1.        , -1.        , -1.        ],\n",
       "         [-1.        , -1.        , -1.        ]],\n",
       "\n",
       "        [[-0.85140216, -0.42732173, -0.53988057],\n",
       "         [-0.81226355, -0.38705367, -0.49328882],\n",
       "         [-0.78176081, -0.36928964, -0.49100071],\n",
       "         ..., \n",
       "         [-1.        , -1.        , -1.        ],\n",
       "         [-1.        , -1.        , -1.        ],\n",
       "         [-1.        , -1.        , -1.        ]],\n",
       "\n",
       "        [[-0.67958099, -0.32375556, -0.4643811 ],\n",
       "         [-0.76858604, -0.40494919, -0.53455555],\n",
       "         [-0.77724749, -0.44441366, -0.55594784],\n",
       "         ..., \n",
       "         [-1.        , -1.        , -1.        ],\n",
       "         [-1.        , -1.        , -1.        ],\n",
       "         [-1.        , -1.        , -1.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.49019611,  0.11372554, -0.52941173],\n",
       "         [ 0.54014039,  0.2656306 , -0.42236209],\n",
       "         [ 0.5938096 ,  0.27554595, -0.54082233],\n",
       "         ..., \n",
       "         [ 0.69967937,  0.03115046, -0.20640022],\n",
       "         [ 0.67181957, -0.03172958, -0.17455906],\n",
       "         [ 0.62208748, -0.23832452, -0.39883339]],\n",
       "\n",
       "        [[ 0.45892847,  0.05119026, -0.54504555],\n",
       "         [ 0.38929236,  0.09078288, -0.49085808],\n",
       "         [ 0.51919603,  0.22890604, -0.57360625],\n",
       "         ..., \n",
       "         [ 0.79965222,  0.1161164 , -0.20289731],\n",
       "         [ 0.57294559, -0.05860406, -0.2512095 ],\n",
       "         [ 0.57628429, -0.16104358, -0.29755259]],\n",
       "\n",
       "        [[ 0.41987014,  0.05877113, -0.33474982],\n",
       "         [ 0.45178914,  0.1450696 , -0.35940206],\n",
       "         [ 0.55995739,  0.1712184 , -0.50154179],\n",
       "         ..., \n",
       "         [ 0.71470678,  0.09857869, -0.30867219],\n",
       "         [ 0.74278557,  0.13935649, -0.11586976],\n",
       "         [ 0.69564617, -0.01932907, -0.14181805]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.05787849, -0.55160248, -0.62321436],\n",
       "         [ 0.27149916, -0.05912578, -0.69014335],\n",
       "         [ 0.24452138, -0.14528137, -0.98496866],\n",
       "         ..., \n",
       "         [ 0.44237816,  0.07349241, -0.768125  ],\n",
       "         [ 0.44443309,  0.08445346, -0.73799902],\n",
       "         [ 0.29518807,  0.0747571 , -0.62519097]],\n",
       "\n",
       "        [[ 0.0348748 , -0.41966021, -0.5934149 ],\n",
       "         [ 0.02126479, -0.35410672, -0.64525402],\n",
       "         [ 0.06131387, -0.30086344, -0.86032188],\n",
       "         ..., \n",
       "         [ 0.16162574, -0.04876649, -0.77930659],\n",
       "         [ 0.34109437,  0.07406557, -0.57467282],\n",
       "         [ 0.52839518,  0.36936855, -0.32082486]],\n",
       "\n",
       "        [[-0.02745092, -0.42745095, -0.78039217],\n",
       "         [-0.16133511, -0.55658728, -0.70450521],\n",
       "         [ 0.02207363, -0.36389267, -0.79841298],\n",
       "         ..., \n",
       "         [ 0.28666902,  0.1281538 , -0.40352708],\n",
       "         [ 0.18431377,  0.04644322, -0.5339489 ],\n",
       "         [ 0.26405799,  0.16154623, -0.31391931]]],\n",
       "\n",
       "\n",
       "       [[[-0.03529406, -0.03529406, -0.03529406],\n",
       "         [-0.07031274, -0.07031274, -0.07031274],\n",
       "         [-0.05263287, -0.05263287, -0.05263287],\n",
       "         ..., \n",
       "         [-0.53415954, -0.53415954, -0.53415954],\n",
       "         [-0.53670388, -0.53670388, -0.53670388],\n",
       "         [-0.55039704, -0.55039704, -0.55039704]],\n",
       "\n",
       "        [[-0.01176465, -0.01176465, -0.01176465],\n",
       "         [-0.04927671, -0.04927671, -0.04927671],\n",
       "         [-0.06454188, -0.06454188, -0.06454188],\n",
       "         ..., \n",
       "         [-0.563079  , -0.563079  , -0.563079  ],\n",
       "         [-0.57229275, -0.57229275, -0.57229275],\n",
       "         [-0.55663991, -0.55663991, -0.55663991]],\n",
       "\n",
       "        [[-0.02278173, -0.02278173, -0.02278173],\n",
       "         [-0.0334869 , -0.0334869 , -0.0334869 ],\n",
       "         [-0.04706067, -0.04706067, -0.04706067],\n",
       "         ..., \n",
       "         [-0.56560516, -0.56560516, -0.56560516],\n",
       "         [-0.55572569, -0.55572569, -0.55572569],\n",
       "         [-0.58266973, -0.58266973, -0.58266973]],\n",
       "\n",
       "        ..., \n",
       "        [[-0.57562995, -0.57562995, -0.57562995],\n",
       "         [-0.58364052, -0.58364052, -0.58364052],\n",
       "         [-0.59528786, -0.59528786, -0.59528786],\n",
       "         ..., \n",
       "         [ 0.35510206,  0.35510206,  0.35510206],\n",
       "         [ 0.53509235,  0.53509235,  0.53509235],\n",
       "         [-0.46289223, -0.46289223, -0.46289223]],\n",
       "\n",
       "        [[-0.6109643 , -0.6109643 , -0.6109643 ],\n",
       "         [-0.61453819, -0.61453819, -0.61453819],\n",
       "         [-0.5874297 , -0.5874297 , -0.5874297 ],\n",
       "         ..., \n",
       "         [ 0.16257763,  0.16257763,  0.16257763],\n",
       "         [ 0.78002858,  0.78002858,  0.78002858],\n",
       "         [-0.33505338, -0.33505338, -0.33505338]],\n",
       "\n",
       "        [[-0.58040553, -0.58040553, -0.58040553],\n",
       "         [-0.61616266, -0.61616266, -0.61616266],\n",
       "         [-0.58646184, -0.58646184, -0.58646184],\n",
       "         ..., \n",
       "         [-0.49885845, -0.49885845, -0.49885845],\n",
       "         [ 0.48916721,  0.48916721,  0.48916721],\n",
       "         [-0.57133955, -0.57133955, -0.57133955]]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python2]",
   "language": "python",
   "name": "conda-env-Python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
